{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The heterogenous causal effect of Bilateral Investment  Agreements on Foreign Direct Investment (FDI): A machine lerning approach\n",
    "\n",
    "# Part 3 of 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent advances in the estimation of heterogenous causal effects build on supervised machine learning methods that have  been traditionally used for predictive tasks. Assuming unconfoundedness, we estimate and compare 4 different models. Each model in implemeted in a dedicated notebook.  \n",
    "\n",
    "\n",
    "Project overview:\n",
    "\n",
    "**Models 1 and 2 : Double Machine Learning causal effect estimation**\n",
    "\n",
    "Under the assumption of unconfoundedness, it is possible to estimate unbiased causal effects of a policy intervention (in our case a Bilateral Investment Agreement) by taking in consideration selection into treatment. Double Machine Learning models consists of 3 main steps:\n",
    "\n",
    "1. Prediction of the presence of a BIT between two countries using a set of socio economic characterisitcs\n",
    "2. Prediction of the FDI flows between the two countries using a set of sociao economic characteristics\n",
    "3. Estimation of the causal effect of the BIT on FDI flows, making use of the difference between the predicted values of the first two steps and their respective observed values.\n",
    "\n",
    "For **Model 1** and **Model 2**, the first two steps are identical, and estimated using a RandomForestClassifier and a RandomForestRegression model, respectively. The two models differ in the third step:\n",
    "\n",
    "\n",
    "* *Model 1: Parametric estimation:* the third step is estimated using a traditional linear regression model.\n",
    "\n",
    "\n",
    "\n",
    "* *Model 2: Non-parametric estimation:* the third step is estimated using a Causal Random Forest\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Models 3 and 4 : Doubly robust  causal effect estimation**\n",
    "\n",
    "Doubly robust methods different from the estimation techniques used in model 1 and 2  in that the prediction of the outcome of interest also uses the treatment variable. The technique used for step 3 of models 3 and 4 therefore match those of models 1 and 2, but they use different predicted values for step 2, and therefore will yield different results.\n",
    "\n",
    "\n",
    "All estimations are made using the **econoML** library:\n",
    "* https://econml.azurewebsites.net/reference.html\n",
    "* https://econml.azurewebsites.net/spec/estimation.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import econml\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Main imports\n",
    "from econml.dml import LinearDML, CausalForestDML\n",
    "from econml.dr import LinearDRLearner, ForestDRLearner\n",
    "\n",
    "# Helper imports\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/desil/Documents/GitHub/EconML')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "# read in data\n",
    "d = pd.read_csv('https://raw.githubusercontent.com/desval/CAS_final_project/main/data/CAS_fdi_panel_complete.csv', sep=',')\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally should use all of the years, but when computing the fixed effects (FE), the dimension increases by a lot and we run into RAM issues. There are approximately 25000 unique country pairs, and therefore the matrix of FE has dimenstion 268000 x 25000. For the time being, reduce the years coverage of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count observations by year\n",
    "d.groupby(['year']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset data according to year\n",
    "# for the moment we use the data in cross-section\n",
    "#d = d[ (d[\"year\"] == 2002) | (d[\"year\"] == 2012) | (d[\"year\"] == 2018)  ]\n",
    "\n",
    "d = d[ d[\"year\"] == 2017  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d[ d[\"stock\"] != 0  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "d_train, d_test = train_test_split( d,\n",
    "                               test_size=0.1,\n",
    "                               random_state=42,\n",
    "                               stratify=d['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outcome of interest is the stock of FDI from source in the destination country. When studying the intensive margin we only consider FDI values that are positive and take the log. Alternatively, use a log trans that preserves the zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log(d_train['stock'].values) \n",
    "plt.hist(y_train)\n",
    "\n",
    "y_test = np.log(d_test['stock'].values) \n",
    "plt.hist(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can study different types of treatment variables:\n",
    "\n",
    "* Binary indicators for the presence of a Bilateral Investment Agreement and a Preferential Trade Agreement\n",
    "* A numeric variable for the depth of a Preferential Trade Agreement\n",
    "* A set of binary variables for the different component of Preferential Trade Agreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A binary treatment for the presence of a bit\n",
    "t_var = [\"bit\"]\n",
    "\n",
    "t_train = d_train[t_var].values.reshape(-1,1) \n",
    "t_test = d_test[t_var].values.reshape(-1,1) \n",
    "\n",
    "print(t_train.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the variables that we want to use in the estimation of the heterogeneous effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable for the heterogeneous effects\n",
    "hetero_vars = [\"gdpcap_o\", \"gdpcap_d\"]\n",
    "\n",
    "X_train = d_train[hetero_vars].values\n",
    "X_test = d_test[hetero_vars].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control variables are socio-economic factors associated with both the probability of receiving the different treatment, and the outcome variables. In addition, we also have to compute the fixed effects, in order to take in accout the panel nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix with controls\n",
    "policy_vars = [ #\"desta_pta\",\n",
    "                \"desta_full_fta\",\n",
    "                \"desta_iprs\",\n",
    "                \"desta_procurement\",\n",
    "                \"desta_standards\",\n",
    "                \"desta_services\",\n",
    "                \"desta_investments\",\n",
    "                \"desta_competition\"]\n",
    "\n",
    "\n",
    "\n",
    "control_drops = hetero_vars + policy_vars + t_var + ['ID_dir'] + ['stock']\n",
    "\n",
    "W_train = d_train.drop(control_drops, axis=1).values\n",
    "W_test = d_test.drop(control_drops, axis=1).values\n",
    "\n",
    "# we also save a vector of names of the featues\n",
    "W_names = np.asarray(d_train.drop(control_drops, axis=1).columns)\n",
    "print(W_names.size)\n",
    "W_names = np.concatenate( (np.asarray(hetero_vars), W_names), axis=None)\n",
    "W_names\n",
    "\n",
    "W_names_out = np.concatenate( ('y', W_names), axis=None)\n",
    "W_names_out\n",
    "print(W_names_out.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(policy_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Fixed effects\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# FE = OneHotEncoder(sparse=False).fit_transform(d['ID_dir'].values.reshape(-1, 1))\n",
    "\n",
    "# we drop one in order to avoid multicollinearity\n",
    "# W = np.append(W, FE[:, 1:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y_train))\n",
    "print(type(t_train))\n",
    "print(type(X_train))\n",
    "print(type(W_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(t_train.shape)\n",
    "print(X_train.shape)\n",
    "print(W_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we also need to prepare the vectors for analyzing the heterogenous effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdpcap_o\n",
    "x_0 = np.array(np.linspace(np.min(X_train[:,0 ]),\n",
    "                           np.max(X_train[:,0 ]),\n",
    "                           15)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdpcap_d\n",
    "x_1 = np.array(np.linspace(np.min(X_train[:,1 ]),\n",
    "                           np.max(X_train[:,1 ]),\n",
    "                           15)).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now need to get all the combinations for the heatmap\n",
    "X_heat = np.array([(x, y) for x in x_0 for y in x_1])[:,:,0]\n",
    "X_heat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we also create some test data by keeping the one of the axis constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for displaying the effects in a plot, with confidence intervals\n",
    "X_test_1 = np.array([(x, y) for x in x_0 for y in x_1[np.argmax(x_1>np.median(x_1))]]) #[:,:,0]\n",
    "X_test_0 = np.array([(x, y) for x in x_0[np.argmax(x_0>np.median(x_0))] for y in x_1]) # [:,:,0]\n",
    "\n",
    "print(X_test_0.shape)\n",
    "print(X_test_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometime we will also need the concatenated X and W (for testing predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_test = np.concatenate( (X_test, W_test), axis=1)\n",
    "XX_test_out = np.concatenate((y_test.reshape(-1,1), X_test, W_test), axis=1)\n",
    "\n",
    "print(XX_test.shape)\n",
    "print(XX_test_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Parametric estimation, doubly robust learning (LinearDRLearner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://econml.azurewebsites.net/_autosummary/econml.dr.LinearDRLearner.html#econml.dr.LinearDRLearner\n",
    "\n",
    "Notes:\n",
    "\n",
    "    model_propensity is the model used to estimate the probability of receiving treatment. The default is ** LogisticRegressionCV**. We will use a random forest classifier instead.\n",
    "\n",
    "    model_regression is the second step, trained by regressing y on the treatment variables, variables used for the heterogenous effects, and controls. We will use RandomForestRegressor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model used to predict the treatment:\n",
    "predict_treatment = RandomForestClassifier(\n",
    "                        n_estimators = 75, # the default is 100\n",
    "                        max_features = 'auto', # the square root of the number of features if auto\n",
    "                        min_samples_leaf = 15, # splits will be considered only if they results in resulting leaves having at least N observations (increase this to avoid overfitting?) defautl is 1\n",
    "                        class_weight = 'balanced_subsample', # if data is unbalanced, computes weights automatically, using sample in each tree (can use 'balanced' to have same weights in all trees )\n",
    "                        bootstrap = True, # sample with replacement\n",
    "                        criterion = 'entropy', # the default is gini\n",
    "                        n_jobs = -1, # how many jobs to run in parallel. Note that fit is parallelized over all trees\n",
    "                        max_depth = None, # None by default. If none split until pure or until hit the min_samples_split\n",
    "                        max_samples = None # number of samples to draw from X, none == X.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model used to predict the outcome based on controls and hetero vars:\n",
    "predict_outcome = RandomForestRegressor(\n",
    "                    n_estimators = 75, # the default is 100\n",
    "                    max_features = 'auto', # the square root of the number of features if auto\n",
    "                    min_samples_leaf = 15, # splits will be considered only if they results in resulting leaves having at least N observations (increase this to avoid overfitting?) defautl is 1\n",
    "                    bootstrap = True, # sample with replacement\n",
    "                    #criterion = 'neg_mean_squared_error', # \n",
    "                    n_jobs = -1, # how many jobs to run in parallel. Note that fit is parallelized over all trees\n",
    "                    max_depth = None, # None by default. If none split until pure or until hit the min_samples_split\n",
    "                    max_samples = None # number of samples to draw from X, none == X.shape[0]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the model to be fit\n",
    "est1 = LinearDRLearner(   model_propensity=predict_treatment,\n",
    "                    model_regression=predict_outcome,\n",
    "                    #discrete_treatment=True,\n",
    "                    fit_cate_intercept = True, # cause effect when hetero vars are zero\n",
    "                    featurizer = None,\n",
    "                    #linear_first_stages=False, # If True predict y with Random regress\n",
    "                    mc_agg='mean', # how to aggregate the nuisance values fron the first stages\n",
    "                    cv=4) # two is the default, could increase it if we have little data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the model\n",
    "est1.fit(y_train,\n",
    "        t_train,\n",
    "        X=X_train,\n",
    "        W=W_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore model results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final model results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(est1_t.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est1.summary(T=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try the model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est1.score(y_test, t_test, X_test, W_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est1.score(y_train, t_train, X_train, W_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model computes a causal effect based on the heterogeneous variables, therefore we can use syntetic values for those values and plot a heatmap of causal effects: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1\n",
    "tau1 = est1.effect(X_heat)\n",
    "lb1, ub1 = est1.effect_interval(X_test, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tau1)\n",
    "plt.hist(tau1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame.from_dict(np.array([np.around(X_heat[:, 0], decimals=3, out=None),\n",
    "                                          np.around(X_heat[:, 1], decimals=3, out=None),\n",
    "                                          tau1]).T)\n",
    "\n",
    "df1.columns = ['X_value','Y_value','Z_value']\n",
    "\n",
    "df1['Z_value'] = pd.to_numeric(df1['Z_value'])\n",
    "\n",
    "df1 = df1.pivot('X_value','Y_value','Z_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1.sort_index(level=0, ascending=False, inplace=True)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(df1,\n",
    "            cmap='coolwarm',\n",
    "            center=0)\n",
    "\n",
    "# Add title and labels to plot.\n",
    "plt.title('Model 3')\n",
    "plt.xlabel('GDP per capita origin (log)')\n",
    "plt.ylabel('GDP per capita destination (log)')\n",
    "plt.savefig(\"charts/heatmap_model_3.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction of treatment performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly we compute some stats about the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the three models from the k fold cross validation\n",
    "est1.models_propensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can print the score of the models\n",
    "print(est1.nuisance_scores_propensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance from the tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est1_t = est1.models_propensity[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "est1_t_predicted = est1_t.predict(XX_test)\n",
    "\n",
    "print(classification_report(t_test, est1_t_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(est1_t.feature_importances_, index=W_names)\n",
    "\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "plt.title('Model 3')\n",
    "fig.set_size_inches(12, 10)\n",
    "feat_importances.nlargest(37).plot(kind='barh')\n",
    "fig.savefig('charts/pred_treat_importance_model_3.png', dpi=100, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot one of the 75 trees that make up the random three:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est1_t_tree = est1_t.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(12, 10)\n",
    "tree.plot_tree(est1_t.estimators_[0],\n",
    "               max_depth = 3,\n",
    "               feature_names = W_names, \n",
    "               fontsize=12,\n",
    "               #class_names=cn,\n",
    "               filled = True)\n",
    "\n",
    "plt.title('Model 3')\n",
    "plt.savefig(\"charts/pred_treat_tree_model_3.png\", bbox_inches='tight')\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display how the model predicts treatment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_test[0,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The force plot shows how the model constructs the prediction for a single observation. In red are features that increase the probability of treatment, in blue the ones that decrease it. The base value is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(est1_t, feature_names=W_names)\n",
    "mean_t_test = np.mean(t_test)\n",
    "# Calculate Shap values\n",
    "choosen_instance = XX_test[0,:]\n",
    "shap_values = explainer.shap_values(choosen_instance)\n",
    "\n",
    "\n",
    "shap.initjs()\n",
    "f = shap.force_plot(#explainer.expected_value[1],\n",
    "                mean_t_test,\n",
    "                shap_values[1],\n",
    "                choosen_instance,\n",
    "                show=True,\n",
    "                #base_value=mean_t_test,\n",
    "               feature_names=W_names) #,\n",
    "               #matplotlib=True)\n",
    "shap.save_html(\"charts/pred_treat_shap_model_3.html\", f)\n",
    "#plt.savefig('scratch.png')\n",
    "f               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction of outcome performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the three models from the k fold cross validation\n",
    "est1.models_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(est1.nuisance_scores_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est1_y = est1.models_regression[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est1_y.feature_importances_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_importances = pd.Series(est1_y.feature_importances_ , index=W_names_out  ) #index=W_names  \n",
    "\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "plt.title('Model 3')\n",
    "fig.set_size_inches(12, 10)\n",
    "outcome_importances.nlargest(37).plot(kind='barh')\n",
    "fig.savefig('charts/pred_outcome_importance_model_3.png', dpi=100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est1_y_predicted = est1_y.predict(XX_test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "# the random data\n",
    "x = y_test\n",
    "y = est1_y_predicted\n",
    "\n",
    "# definitions for the axes\n",
    "left, width = 0.1, 0.65\n",
    "bottom, height = 0.1, 0.65\n",
    "spacing = 0.005\n",
    "\n",
    "\n",
    "rect_scatter = [left, bottom, width, height]\n",
    "rect_histx = [left, bottom + height + spacing, width, 0.2]\n",
    "rect_histy = [left + width + spacing, bottom, 0.2, height]\n",
    "\n",
    "# start with a rectangular Figure\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.xlabel('Outcome (log FDI stocks)')\n",
    "plt.ylabel('Predicted outcome (log FDI stocks)')\n",
    "\n",
    "ax_scatter = plt.axes(rect_scatter)\n",
    "ax_scatter.tick_params(direction='in', top=True, right=True)\n",
    "ax_histx = plt.axes(rect_histx)\n",
    "ax_histx.tick_params(direction='in', labelbottom=False)\n",
    "ax_histy = plt.axes(rect_histy)\n",
    "ax_histy.tick_params(direction='in', labelleft=False)\n",
    "\n",
    "# the scatter plot:\n",
    "ax_scatter.scatter(x, y)\n",
    "\n",
    "# now determine nice limits by hand:\n",
    "binwidth = 0.25\n",
    "lim = np.ceil(np.abs([x, y]).max() / binwidth) * binwidth\n",
    "#ax_scatter.set_xlim((-lim, lim))\n",
    "#ax_scatter.set_ylim((-lim, lim))\n",
    "\n",
    "bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "ax_histx.hist(x, bins=bins)\n",
    "ax_histy.hist(y, bins=bins, orientation='horizontal')\n",
    "plt.title('Model 3')\n",
    "ax_histx.set_xlim(ax_scatter.get_xlim())\n",
    "ax_histy.set_ylim(ax_scatter.get_ylim())\n",
    "\n",
    "\n",
    "\n",
    "#plt.show()\n",
    "plt.savefig('charts/pred_outcome_scatter_model_3.png', dpi=100, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
